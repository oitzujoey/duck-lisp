
Goal: Small scripting language that can be embedded anywhere, though not necessarily with the compiler attached.
Features:
	Lisp syntax
	No dependency on any library.
	C/C++
Drawbacks:
	Lisp syntax
	Redundant code due to lack of libraries.

Types
	constant
	name
	function
Composite Types
	enum
	array
	struct
	union

Objects
	int
	bool
	enum
	array
	string
	struct
	union
	function

There will be a parent table called _. This table is the parent of all global variables.
There is no type difference between a string and an identifier. A string constant can contain any character. An identifier must start with a letter or identifier_symbol and the rest of the characters must be letters, identifier_symbols or numbers.
If an identifier is passed to a expression or subexpression, the value will be used in the operation if possible. If the identifier cannot be found in the variable list, then the identifier will be converted into a string literal.
Every expression is a table. When executing a table, the first value must be a function.
("a" "b" "c") is {"a", "b", "c"}.

((a ('a' 'b' 'c')) 'b' 'c')

a/b/c
a/b:

When calling a function, there are two tables. The first is the table that passed to the function. The second is the table that defines how the function is called.

[function f [a b c] [
	[return [+ a b c]]
]]
[f 1 2 3]

The calling list is f [1 2 3]
The definition list is f [a b c]
This is the same call expressed in a different way: [f f.b:2 f.c:3 f.a:1], or [f .b:2 .c:3 .a:1] (shorthand) This can be represented in the AST, but it will not necessarily execute sucessfully if the members are defined outside the function call.

evaluateSubexpression -> Returns a bool, an int, a float, a string, or an identifier.
identifierToValue
stringToIdentifier


"ABI"
	before
		stack base
		args length (always > 0) (DuckLisp FP points here)
		function name (string)
		arg1
		arg2
		arg3
		arg4
		...
		loc0
		loc1
		loc2
		loc3
		...
	after
		stack base
		args length (always > 0) (DuckLisp FP points here)
		function name (string)
		arg1
		arg2
		arg3
		arg4
		...
		loc0
		loc1
		loc2
		loc3
		...
		ret0
		ret1
		ret2
		ret3
		...
		rets length

Data tree
	Access by name is fast.
	Hashes of a given string must always return the same value no matter the scope.
	An object's parent object is its namespace.
	An object may have children.

Can have two methods of execution
	Tree walk
	Virtual machine

Function types
	Bytecode generation
		Appends or inserts bytecode into the block.
	C callback
		Inserts C function call into block in bytecode wrapper.
		This class of functions is called by a bytecode generation function.
	Heterogeneous
		Calls bytecode and callback functions.


nop
add8
add16
add32
add64
sub8
sub16
sub32
sub64
push
pop
return



#call bytecode
#call *bytecode
#call C
#call *C
#add *float *float
#add *float float
#add *int *int
#add *int int
#sub *float *float
#sub *float float
#sub *int *int
#sub *int int





It turns out that this was not a bug in the allocator. Hurrah!
                (dl_memoryBlock_t) { /* 90 */
                        .block = 4A690AF, /* offset = 111 */
                        .block_size = 2,
                        .allocated = true,
                        .unlinked = false,
                        .previousBlock = 44,
                        .nextBlock = 94,
                },

                (dl_memoryBlock_t) { /* 44 */
                        .block = 4A690AE, /* offset = 110 */
                        .block_size = 1,
                        .allocated = true,
                        .unlinked = false,
                        .previousBlock = 42,
                        .nextBlock = 90,
                },


I'm going to split the compiler and VM to a certain extent. DuckLisp will always have the VM, but it won't always have the compiler. This will allow me to use as little memory as possible on small devices. For example, I doubt MicroComp will be able to compile DuckLisp for a while, even after a C compiler is ported to it.

Constant propigation
	All the function has to do is be marked as pure. If it is pure and all arguments are constants, then the function may be pre-calculated in that instance.
Tail call
	Is a recursive call the last function called? Replace the arguments and jump to the beginning.


Bookmark:
	Figure out how bytecode chunks are structured in relation to each other before they are merged.
		Function definitions can all be placed at the end of the program.
		Function calls will only be generated after the function has been defined.
	Make `compile` generate function call bytecode and call generators.
	I have encountered a need for temporary variables. The question is, how do I allocate and free them? Once I've solved that, is there an easy way to do it more efficiently?
	The problem with recursion is lack of context. With a stack, I can just pop the last value to see the context. With a function call, I can't see anything above the current function.

--- Two months later ---

^^^ Helpful, but I'm still confused.
Stack is for placing objects on during program execution, right?
Scope stack is for placing symbols on during program compilation.
	These symbols are variables and generators.
My code does not entirely reflect the above.

I seem to have VM functions that manipulate the stack. If I am to completely seperate the VM from the compiler, then I need to move them to the VM module. C callbacks should not be passed to the compiler. The compiler should return a mapping of names to callback slots, and the callbacks should be placed in those slots during VM initalization. These slots will likely be stored on the stack.

Generators will generate high-level bytecode. This bytecode is inserted into a linked list that doubles as an array. Branch targets will be stored as indexes to the array, which will allow instructions to be inserted between the branch and the target with no side effects.
Functions are compiled to bytecode, and placed after the `defun` instruction. `defun` then pushes the address of the function on the stack. `call` will take the stack address and extract the address of the function. The function will then be called.
C functions are not stored in bytecode. They are stored on a separate stack dedicated to C callbacks. The `ccall` instruction will take the callback stack address and then run the function in that position. Forward references to a label are dealt with by inserting a pseudo-instruction to act as the target. When the bytecode emitter comes across this pseudo-instruction, it immediately replaces it with the first of its own instructions.

To ease the load on my brain, I think I will add an intermediate representation. This is solely for the purpose of dealing with branch targets.

Compiler sees expression. Compiler looks up function name in the compiled function list. If it is there, a function call is generated. Compiler looks up function name in C function list. If it is there, a C function call is generated. Compiler looks up name in generator function list. If it is there, the compiler passes the expression to the generator. The generator emits bytecode. If a generator does not exist, a compile error is thrown.

Recursive descent could work for optimization. A generator can not see anything above it, but it can see below it. The generator can traverse down the tree as far as far as it desires, and if it sees an optimization, it can rearrange the tree. The generator will then return and the compiler will traverse the tree by one node. It will then call the generator for that function. This will continue until the whole tree has been traversed.

Each high-level bytecode instruction will contain an opcode class and arguments. An opcode is an enum. Arguments are unions that can have the types integer, float, string, or label. Labels are `ptrdiff_t`s that point to an element of the bytecode array.

High-level bytecode is assembled into raw bytecode. The opcode is determined from the opcode class and argument sizes, and branch targets are calculated from the final instruction lengths.

Perhaps I should create generators that emit a single instruction so that it is possible to write VM assembly directly in the program? I could also add an assembler that accepts text assembly files. I doubt they would be too hard to parse.

If I go with all of the above, we will have these modules:
	DuckLib.so
	duckLisp.so
	duckVM.so
	duckAsm.so

We traverse the tree top-down because we want to allow the generators to optimize the the tree if they wish.

There are three stacks. "The Stack" is the runtime VM stack. The generator stack contains generators. The function stack may contain C functions.

All functions are anonymous in the VM.

Tree ‚Üí list strategy:
	Generator:
		Check arguments.
		Reorder arguments in tree.
		Create list of instruction list fragments. These lists will be joined with the compiled arguments, which may themselves have similar trees of instruction list fragments. It looks sort of like this:
			ast = [a, b, c]
			generator:
				instructionList = [i0, i1, i2]
				order = [2, 1, 3]
				reorder(ast, order)
			for i in range(len(ast)):
				newInstructionList.append(instructionList[i])
				newInstructionList.append(ast[i])
		Expand tree into instruction list. This is easy, since we just traverse the tree and append each leaf to the end of the list.

bytecode file format:
	((ascii8[2] DL) (uint16 <callbacks length>) (uint32 <bytecode length>) (uint8[<bytecode length>] <bytecode>))

[[i0 i1 i2] [i3 i4 i5] [i6 i7 i8] [
	[[i9 i10 i11] [i12 i13 i14] [i15 i16 i17] [
		[[i18 i19 i20] [i21 i22 i23]]
	]]
	[[i24 i25 i26] [i27 i28]]
]]

node = [[instruction*]* [node*]]

instructions = [instruction*]
instructionsList = [instructions*]
nodes = [node*]
node = [instructionsList nodes]

node = dl_array_t:dl_array_t
nodes = dl_array_t:dl_array_t
instructionsList = dl_array_t:dl_array_t
instructions = dl_array_t:duckLisp_instructionObject_t
instruction = duckLisp_instructionObject_t

Almost done with compilation. I have decided on the strategy of giving each generator its own piece of the instruction list. The problem is that arguments must be evaluated left-to-right to allow the program to execute top-down.

(7 (3 (1) (2)) (6 (4) (5)))

(
  (defun copy-function ((f pointer:function) (size size_t))
    (var g pointer:function)
    (setq g (malloc size))
    (copy-bytes (cast g pointer:byte) (cast f pointer:byte) (* size size-of(byte)))
    (return g))

  (defun f () 0)
  (var p-g pointer:function)
  (copy-function p-g (addr-of f))
  (defmacro g ()
    #(call p-g))
  (g))

(
  (defun # #
    (var # #)
    (setq g (malloc size))
    (copy-bytes (cast g #) (cast f #) (* size (size-of byte)))
    (return g))

  (defun f # #)
  (var # #)
  (copy-function p-g (addr-of f))
  (defmacro # #
    '(call p-g))
  (g))

(
  (defun
    (var)
    (setq g (malloc size))
    (copy-bytes (cast g) (cast f) (* size (size-of byte)))
    (return g))

  (defun f)
  (var)
  (copy-function p-g (addr-of f))
  (defmacro
    '(call p-g))
  (g))

(
  (defun
    (var)
    (setq g (malloc size))
    (copy-bytes (cast g) (cast f) (* size (size-of byte)))
    (return g))

  (defun f)
  (var)
  (copy-function p-g (addr-of f))
  (defmacro
    '(call p-g))
  (g))

(() ()):    Left-right - 
(f () ()):  Undefined  - Whatever
(f (g ())): Outside-in - Tree, top-down, right-left

Outer expressions compile before inner.
Inner expressions' assembly come before outer's.
Left expressions' assembly come before right's.

Outer expressions compile before inner.
Inner expressions' assembly comes after outer's.
Left expressions' assembly comes before right's.

(7 (3 (1) (2)) (6 (4) (5)))
(1 (5 (7) (6)) (2 (4) (3)))

(1 (2 (3) (4)) (5 (6) (7)))
(1 (5 (6) (7)) (2 (3) (4)))
(1 (5 (7) (6)) (2 (4) (3)))

(5(2 (*1) (*1)) (4 (*3) (*3)))

(7 (3 (1) (2)) (6 (4) (5)))
(7 (3 (1) (2)) (6 (4) (5)))


(7 (3 (1) (2)) (6 (4) (5)))

7 +
 3 +
  1 +
  2 -
 6 -
  4 +
  5 -

Required objects:
	Return stack
	Index? May be built into the array.
	Current node

+ Create array. Push array in current node. Push array on stack. Set array as current node. Push instruction list in current node.
0 Push instruction list in current node.
- Push instruction list in current node. Pop array from stack. Set current node to popped array?

+ Create array. Push array in current node. Push array on stack. Set array as current node. Push instruction list in current node.
0 Push instruction list in current node.
- Push instruction list in current node. Pop array from stack. Set current node to popped array?

I will need a local struct for this.

struct node_s {
	union {
		dl_array_t *node;
		dl_array_t *instructions;
	}
	dl_bool_t isNode;
} node_t;


rrealloc: Instead of adding memory to the end of the block, it adds it to the beginning.


Move pusher to top of loop. Done.
Generate tree in pusher.

node
  instructions
  node
  node
  node

First element in node is always an instruction array.
Store node addresses on stack, not nodes.

We have a dedicated node struct now.
All nodes will be kept in a dl_array.
Nodes will keep an array of nodes by storing the index in the master array.
Indices will be pushed on the stack.

Needed universal identifier for each node ‚Üí index to a single array.
Needed each nodes to contain a list of nodes ‚Üí indeces to a subset of the array elements.
Desired a way to keep track of all nodes in use ‚Üí single array containing nodes.
Desired a way to use dl_array instead of raw memory allocation.


Whew! That's done.


The VM is a modified stack machine. It might be more helpful to think of it as a Harvard architecture machine with a data memory that can grow infinitely.
Each function call creates its own environment. This is mainly a feature of the compiler. When a function is called, local variables are pushed onto the stack. At the end of the function they are popped. DL functions are stored in program memory. Functions are DL objects that contain either a pointer to a DL function or a pointer to a C callback. Since all objects are stored on the stack, all functions are local variables that can be pushed and popped. Variables are referenced by index relative to the current frame pointer. The reason for this is that the stack pointer changes too often to easily keep track of indexes using it, and absolute addressing takes up a lot of space. In most cases, the frame pointer will remain constant for the entire duration of a function. If another function is called, the callee will need to use an offset relative to the frame pointer of the scope that contains the function.

stack ‚Äî variables
variable ‚Äî generic data

"ABI"
	before ‚Äî Push, call, push
		stack base
		args length (always > 0) (DuckLisp FP points here)
		function name (string)
		arg1
		arg2
		arg3
		arg4
		...
		loc0
		loc1
		loc2
		loc3
		...
	after ‚Äî Push
		stack base
		args length (always > 0) (DuckLisp FP points here)
		function name (string)
		arg1
		arg2
		arg3
		arg4
		...
		loc0
		loc1
		loc2
		loc3
		...
		ret0
		ret1
		ret2
		ret3
		...
		rets length
	cleanup ‚Äî Return, copy
		ret0 ‚Üê Rets are now locs owned by the caller
		ret1 ‚Üê Rets are now locs owned by the caller
		ret2 ‚Üê Rets are now locs owned by the caller
		ret3 ‚Üê Rets are now locs owned by the caller
		arg3 ‚Üê Stack top
		arg4
		...
		loc0
		loc1
		loc2
		loc3
		...
		ret0
		ret1
		ret2
		ret3
		...
		rets length

Calls to DL from C are done by pushing objects on the stack and then calling the function. Return values are placed on the top of the stack for C to pop.
The stack is persistent between calls.
Bytecode is not persistent between calls. Bytecode can be run straight by the VM, or a function that is currently on the stack can be called. A bytecode function pointed to by the stack will point to a random block of memory. C will be given no indication of when it is freed.

This is a one-shot run of some bytecode. This may define functions that can be used by other chunks of bytecode. Dangling pointers may result if `bytecode` is freed.
e = duckVM_execute(&duckVM, bytecode, bytecode_length);

This is a function call. DVM knows nothing about function names, so an index must be given instead.
e = duckVM_pushObject(&duckVM, duckLispObject);
e = duckVM_call(&duckVM, function_index); // Index is an absolute address on the stack.
e = duckVM_popObject(&duckVM, &duckLispObject);

Solved:
	C ‚Üí DL calling conventions.
	DL ‚Üí C/DL calling conventions?
	Local variables.
Unsolved:
	Non-local scope addressing. Real CPUs deal with this by storing functions in memory, not on the stack.


A label may only have one destination address.
A label may have many source addresses.

A label will have its index placed in a trie right after it is assembled to bytecode.
A backward goto will calculate its jump distance when it is converted to bytecode.
A forward goto will use 32 bits and enter its address into a trie (with label as key) that points to an array with pointers to all the gotos to that label.

(goto <label>) ‚Äî Create label and goto array if not already created. Emit high-level jump instruction.
(label <label>) ‚Äî Create label if not already created. Emit label pseudo instruction.
label trie points to an array of links?
Jump ‚Äî Insert current address in car of link array.
Label ‚Äî Insert current address in cdr of link array.

Goto trie will contain pointers to arrays of source addresses.
Label trie will contain destination addresses.
After bytecode list is generated and goto and label tries are populated, create jump link array.

I did the label scoping wrong. Scoping should always be done in the generators, but I did it in bytecode generation. What I am currently doing is giving the label name (a string) as the instruction argument. What I should be doing is looking up the label index in the label trie (which may have been constructed in the same generator). The index, not the name, should be passed as the instruction argument. During code generation, the index is used to populate the element of the label array that contains the jump structure.

I dealt with the label scoping problem. It works better. The problem I'm having now is again forward references. Apparently forward references are a problem for scoping in addition to bytecode generation. If a goto or label (they are the essentially the same) is placed inside a scope and another goto or label has been placed after the scope, two labels will be created, one inside and one outside. If a goto or label preceeds the scope, only one label is created.

Rules:
	A label returns no value. It is a pseudo-instruction.
	A label may only appear in the top-level array of a closed scope expression.
		Allowed:
			(
			  (label "cleanup"))
		Disallowed:
			(+ a b (label "cleanup"))
			(push-scope (label "cleanup"))
		This rule is partially an extension of the first rule. This allows the () expression to create the label before its children search for it.

Escape sequences are now expanded in CST ‚Üí AST.


Discord Hanabi chat:
<discord user=an_origamian>
I've been working on branching.
Scoping for it is done and wasn't too hard, though the current implementation only allows labels in the top level of an expression with an expression as its function part.
```lisp
;; Labels allowed here.
(
  (label l)
  (nop)
  (goto l))

;; Labels disallowed here.
(+ (label l) 5 (goto l))

;; Labels disallowed here.
(+ (push-scope)
   (label l)
   (nop)
   (goto l)
   (pop-scope))

;; Labels allowed here.
(+ (
     (label l)
     (nop)
     (goto l))
   5
   6)
```
.
Labels are pseudo-instructions that have no bytecode representation.
Gotos are jump instructions.
At the moment, labels and gotos return no value.
```lisp
;; Jump8 uses a one-byte relative address.
(jump8 offset) ; Two bytes long.
;; Jump16 uses a two-byte relative address.
(jump16 offset-LSB offset-MSB) ; Three bytes long.
;; Jump32 uses a four-byte relative address.
(jump32 offset-LSB offset-ML offset-MH offset-MSB) ; Five bytes long.
```
When a jump instruction executes, the instruction pointer (IP) points first to the opcode and then to each of the bytes in the operand. After the relative address has been extracted from the instruction, the IP points to the instruction after the jump. The relative address is then added to this value of the IP, and the VM continues executing code from the new address.
.
The compiler portion of the `compile` function generates high-level assembly. Instead of emitting a jump8 instruction, it would simply emit the jump high-level instruction. This then gets converted to one of the above three variants of jump by the assembler portion of `compile`. The question is, how does the assembler choose which instruction to emit? It seems simple at first. Just subtract the source of the jump from the target label. If all jump instructions are the five-byte form, this works just fine, but if the jump instructions vary in length, then it becomes more difficult to calculate the real target address. Take this for example:
```lisp
(
  (label l)
  <130 nops>
  (goto l))
```
This is compiled to (using a representation of the internal high-level assembly)
```lisp
(
  (label 0)
  <130 nops>
  (jump 0))
```
`0` is the index of the label in the label array. It does not correspond to the actual address in any way.
And now down to bytecode:
```lisp
0   <130 nops>
130 <jump8, 16, or 32?>
```
To determine the size of the instruction, we need to know the jump distance. This is easy. Distance ‚âà 0 - 130 = -130. |-130| > 127, so we need to use the 16-bit version. The distance is measured from the last byte of the jump instruction, so we also have to add that to the naive total. -130 - 3 = -133.
```lisp
0   <130 nops>
130 jump16 16'd-133
```
`16'd<number>` is Verilog's notation for 16-bit decimal numbers. It's quite convenient for this sort of thing.
Easy! It's just a bit of arithmetic.
Here's where it gets complicated.
```lisp
(
  (label l)
  (label m)
  <124 nops>
  (goto l)
  (goto m))
```
‚Üì
```lisp
(
  (label 0)
  (label 1)
  <124 nops>
  jump 0
  jump 1)
```
‚Üì
```lisp
0     <124 nops>
124   jump?? ??
124+? jump?? ??
```
Now how do we calculate this? It's not that much more difficult to figure out, but if we focus on the second jump it illustrates the point. If we assume the first jump is jump8, then the whole instruction will be two bytes long. If we assume the second jump is jump8, it will also be two bytes. Label `m` will be 0-(124+2+2) = -128 bytes forward. Since the size of the address fits in eight bits using two's compliment negative numbers, jump8 is sufficient to reach the target.
```lisp
0   <124 nops>
124 jump8 8'd-126
126 jump8 8'd-128
```
And it works out! This only happened because of the assumption that each offset fit in a single byte. If we use jump16, it no longer fits.
```lisp
0   <124 nops>
124 jump16 16'd-127
127 jump16 16'd-130
```
It would be possible for the assembler to optimize this to use jump8s, but assuming the smallest size instruction and working up to largest is much easier to program.
.
Needless to say, most programs are going to have a ton of branches, and determining the size will not necessarily be easy.
One approach is to iterate over all the branches in the bytecode and gradually expand each one until the offsets of each one fit into the instruction. This requires that extra bytes are inserted into the middle of the bytecode. To do this with an array, the bytecode memory block would have to be reallocated and then a portion of the bytecode would have to be copied to make room for the inserted instruction. This is inefficient. Another approach is to make the bytecode an array containing links in a list. This has the advantage that links can be inserted into the middle of the list and each link can still be accessed by its index in the array. New list links are pushed onto the end of the array. The problem is that since incrementing the array index does not necessarily mean incrementing the list index, distance can no longer be determined by subtracting the source index from the target index. A third approach is to create a system of equations describing the relationships between each branch. Once the equation is solved and addresses are mathematically calculated, distance is no longer required, so all that needs to be done is rewrite the jump opcodes and insert links after them with the correct address. I don't think creating the equation will be hard, but solving it will be another matter since it will either have if statements or logs.
I've read that optimizing the code size of addressing is not something that programmers worried about even in the 70s, but I still think it would be nice to have.
.
`asize(a)` = `ceiling(log128(a)) + 1`
```c
asize(0) = 0
asize(1) = 1
asize(-1) = 1
asize(127) = 1
asize(128) = 2
asize(-128) = 1
asize(-129) = 2
```
`index` = Index of byte in bytecode array before addresses are inserted.

Source:
```lisp
(
  (label l)
  (label m)
  <124 nops>
  (goto l)
  (goto m))
```
Assembled bytecode:
```txt
<124 nop opcodes>
; No operand yet. Just opcode.
jump8
jump8
```
Annotations:
```txt
l0
l1
    <124 nops>
b0  jump l0 a0
b1  jump l1 a1
```
Left field is an address.
Middle field is an opcode.
Middle-left field is a label so you know where it goes.
Right field is an address associated with the opcode. It is not actually in the bytecode array.
This is the system of equations that result.
```c
l0 = index
l1 = index
b0 = index
a0 = l0 - (b0 + asize(a0))
b1 = b0 + 1 + asize(a0)
a1 = l1 - (b1 + asize(a1))
```
This expands to
```c
l0 = 0
l1 = 0
b0 = 124
a0 = l0 - (b0 + asize(a0))
b1 = b0 + 1 + asize(a0)
a1 = l1 - (b1 + asize(a1))
```
```c
a0 = 0 - (124 + asize(a0))
b1 = b0 + 1 + asize(a0)
a1 = 0 - (b1 + asize(a1))
```
```c
a0 = -ceiling(log128(a0))      - 125
b1 =  ceiling(log128(a0))      +   2
a1 = -ceiling(log128(a1)) - b1 -   1
```
Wheeeee!!!! Logs and ceilings, and maybe even if statements! I have no idea how to solve this without a trial and error approach. Trial and error may be an acceptable solution.
</discord>

1. Start with best estimate using 2-byte jump instructions.
2. Clear the "not done" flag. Set the cumulative offset to zero. For each link‚Ä¶ (in order)
  a. Add cumulative offset.
  b. Calculate required instruction size.
  c. Set "not done" flag if current instruction size is too small.
  d. Increase instruction size to required instruction size if necessary.
3. If "not done" flag is set, goto 2. Else, goto 4.
4. Reallocate the bytecode to account for the new size of the branches.
5. Write the branch instructions and relocate the incumbent bytecode.

Sort:
	One array is sorted.
	One array is unsorted.
	The destination array is double the size of the other two.

Array is filled and sorted.


I'm writing generators now. They are a pain, simply because dynamic typing requires detecting many types.
Solutions:
1. Replace the type tree with a single type struct. Will require lots of work, but will reduce the code complexity throughout duck-lisp.
2. Make a list of DL API functions. Refer the chart when writing generators.

Did #1.
Writing generators should be a bit easier now.

The iterative tree traversal is a pain. Let's try making it recursive.

The compiler will change.
The assembler may change.
The generators will change.
The parser will not change.
The disassembler will not change.
The emitters should not change.

Steps:
source ‚Üí reader ‚áí AST
(AST ‚Üí generator ‚áí AST) & (AST ‚Üí emitter ‚áí assembly)
assembly ‚Üí assembler ‚áí bytecode
bytecode ‚Üí disassembler ‚áí disassembly

Code structure:
loader
    reader
	compiler
	    generator
		    generator
			emitter
	assembler
	optimizer
disassembler

Fibonacci: {

push-integer.8  00        ; (var a 0)
push-integer.8  01        ; (var b 1)
push-integer.8  00        ; (var c 0)
                          ; (label loop)
push-index.8    00        ; (print a)
c-call.8        00
pop.8           01
push-string.8   01 "\n"   ; (print "\n")
c-call.8        00
pop.8           01

add.8           00 01     ; (+ a b)
move.8          03 02     ; (setq c %)
pop.8           01
move.8          00 01     ; (setq b a)
move.8          02 00     ; (setq a c)

push-index.8    00        ; (print a)
c-call.8        00
pop.8           01
push-string.8   01 "\n"   ; (print "\n")
c-call.8        00
pop.8           01
push-integer.16 E803      ; (print 1000)
c-call.8        00
pop.8           01
push-string.8   01 "\n"   ; (print "\n")
c-call.8        00
pop.8           01

push-integer.16 E803      ; (< 1000 a)
less.8          03 00
c-call.8        00        ; (print %)
brnz.8          C0        ; (brnz % loop)
pop.8           02

nop                       ; (nop)

jump.8          BB        ; (goto loop)

return
}

I ran into a problem I didn't expect. In order to calculate the condition for a branch, I need to push objects on the stack. In order to keep the stack balanced, I need to pop those items off the stack. So I guess I will do something similar to what I did to `progn` and pop all arguments required in the calculation before the branch. The number of arguments to pop will be given to `br??` as an additional integer argument. This argument will probably have to come after the jump offset in order to remain compatible with the current jump size optimization.

What the final version will look like:


push-integer.16 E803      ; (< 1000 a)
less.8          03 00
c-call.8        00        ; (print %)
+brnz.8          C0 02     ; (brnz % loop)
-brnz.8          C0        ; (brnz % loop)
-pop.8           02
-
-nop                       ; (nop)
-
-jump.8          BB        ; (goto loop)

The top scope seems to always be unused. This should not be deleted without investigation because it may be possible for the top-level expression to declare a local.

Woohoo! Branching works!

label 0 multiply-loop
goto  1 multiply-end
label 2 add-loop
goto  3 add-end
label 3 add-end
goto  2 add-loop
goto  0 multiply-loop
label 1 multiply-end

label multiply-loop
goto  multiply-end
label add-loop
goto  add-end
goto  add-loop
label add-end
goto  multiply-loop
label multiply-end

Problem fixed by making labels greater than gotos when a tie occurs when sorting.

Macros:
	Create VM.
	Push arguments as locals.
	Run macro.
	Retrieve and paste result.
	Destroy VM.

Functions:
	Save PC on call stack.
	Push arguments as locals.
	Jump to function.
	Run function.
	Set PC to top of call stack, pop all function arguments and locals, and push the result.

Functions will be placed in the bytecode in relation to where they were defined in the source. The first instruction of a function will be a jump to the end of the function. This is so that program flow can pass through a function without running it. It *is* a hack, but right now I'm probably too lazy to do anything more complicated like relocating the function body.

(defun 1+ (a)
  (print a)
  (print (+ a 1)))
(print (1+ 5))

(goto --g###1)
  label 1+
  push-integer 1

  (print -1)

  add -2 -1
  
  (print -1)
  
  return 2
(label --g###1)
push-integer 5
call 1+
ccall print


All stack addressing is relative to the frame pointer.

Bookmark:
    Add implicit progn to function body.
    Return instruction
	    pop
		reset frame pointer
		return
	call instruction
	    save frame pointer
		jump

Bookmark:
	Use frame pointer.
	Find error messages that don't actually throw an error and force them to.

FUNCTIONS WORK!!!

REPL time!
I need to free my memory. Here's the list of all resources:

main::duckLispMemory freed
duckLisp_init::duckLisp->source Needs to be reallocated in duckLisp_loadString and freed in duckLisp_quit
duckLisp_init::duckLisp->errors Needs to be reallocated in duckLisp_loadString and freed in duckLisp_quit
duckLisp_init::duckLisp->scope_stack Needs to be reallocated in duckLisp_loadString and freed in duckLisp_quit
duckLisp_init::duckLisp->generators_stack Needs to be freed in duckLisp_quit
duckLisp_init::duckLisp->labels_stack Needs to be reallocated in duckLisp_loadString and freed in duckLisp_quit
I think cst_append is fine.

duckLisp_quit needs to clean up a bunch of stuff.

Labels will need to be able to return absolute program addresses.

(defun length (list)
  (var i 0)
  (while (not (null? list))
	(setq list (cdr list))
    (setq i (1+ i)))
  i)

(defun nreverse (list)
  (var reversed-list (list))
  (while (not (null? list))
	(setq reversed-list (cons (car list)
							  reversed-list))
	(setq list (cdr list)))
  reversed-list)

(defun append (list1 list2)
  (var appended-list (list))
  (while (not (null? list1))
	(setq appended-list (cons (car list1)
							  appended-list))
	(setq list1 (cdr list1)))
  (while (not (null? list2))
	(setq appended-list (cons (car list2)
							  appended-list))
	(setq list2 (cdr list2)))
  
  (nreverse appended-list))

;; Edit: I don't believe this will work. It can capture variables, but it will create a copy instead of pointing to it.
(defmacro lambda (caps args &rest body)
  (var name (gensym))
  `(no-scope
	(defun ,name ,args
	  ,body)
	(list (get-label ,name)
		  ,(append args caps))))

;; This is complicated for a simple function call‚Ä¶
(defun funcall (function arguments)
  (var label (car function))
  (var args (car (cdr function)))
  (var args-length (length args))

  (while (not (and (null? args) (null? arguments)))
	(push (if (null? (car args))
			  (
			    (var result (car arguments))
			    (setq arguments (cdr arguments))
			   result)
			  (car args)))
	
	(setq args (cdr args)))

  (call label))

Required keywords/functions
    lambda
        defmacro
	    gensym ‚Äî partially implemented
		quote ‚Äî Implemented
		list ‚Äî Implemented
		    cons ‚Äî Implemented
		get-label ‚Äî Implemented
	funcall
	    car ‚Äî Implemented
		cdr ‚Äî Implemented
		null? ‚Äî Implemented
		push ‚Äî partially implemented
		call ‚Äî Implemented

(setq a-closure (list func a b c))

move.8 is throwing an error. This is resolved and was probably caused by bad balancing.

It would be *nice* to intern symbols, but I don't know how easy that would be. If I omit symbols, string literals could be a problem, but then again, I could just put them inside strings. It shouldn't be too hard to detect a string literal. The only problem this causes is when algorithmically creating string literals, but in this case, just wrap quotes around the string to distinguish it.

So here are the disadvantages of not having a dedicated symbol type:
    Symbol comparison is slow.
	The difference between symbols and strings is that symbols have a specific string format.

There is now a new symbol data type. It does not yet have a master package. Edit: Now it does.

get-label prerequisites:
    Fetch absolute address of labels. I may have to do more fixups. On the other hand, these addresses are absolute, so that means four bytes for every one of them.
	Fetch labels by name, ID, or pointer. Labels lose all of their identity when they are copied into the links array, and multiple target duplicates are created.

I need to keep track of
    Absolute address of target
	Absolute address of push-integer.32

number of targets != number of push-integers
number of links > number of targets
number of links != number of push-integers

Easy solution: Add an array to certain links, the elements of which point to the push-integer.32s.
Slightly more difficult solution(?): Create a target-reference link array.

Create a second set of links which point from the `push-integer.32`s to the targets?
I think all I have to do is set `.size` to 4 and the optimizer will figure everything out for me.

I could do something really hacky. üòÄ Subtract 4 from "push-integer.32" and then set `.size` to 4 before optimization. 4 will be added to the instruction during optimization, which will change the instruction back to "push-integer.32".
The proper way would be to add another boolean to `jumpLink_t` that keeps it from being optimized and treats it as if there's a size of 4. This should be slightly slower.

Absolute addressing routed using boolean.

Callbacks implemented.

Macros will be a special case of generators. There will be a dedicated generator macro that will look at its name and then call the macro script associated with it.

Lambdas are about twice as complicated as I expected. Apparently I need to capture *the original variable*, not a copy.

defun f x
        [Œª y
          (funcall y x)
       	 Œª z
       	   ‚Üê x z]
(funcall f 5)

compiles to

    f: jump e0				  defun
   Œª0: jump e1				  defun‚Üílambda0
       funcall y			  defun‚Üílambda0‚Üífuncall‚Üíexpression
	   return 1				  defun‚Üílambda0
  $c0: push-closure Œª0 1 x 	  defun‚Üílambda0
e1,Œª1: jump e2				  defun‚Üílambda1
       set-uv ux z			  defun‚Üílambda1‚Üíexpression
	   return 1				  defun‚Üílambda1
  $c1: push-closure Œª1 1 x	  defun‚Üílambda1
   e2: push-list 2 $c0 $c1	  defun
       make-uv 1 x			  defun
	   return 3				  defun
e0,$x: push 5				  funcall
       call f				  funcall

New functions:
    push-closure Œª0 1 x ‚Äî stack.append('(Œª0 &x))
	call c ‚Äî (let ((a (cdr c)))
	           (while a
	             push (car a)
				 ‚Üê a (cdr a))
			   jump (car c))
	set-uv i x ‚Äî ‚Üê uv[i] x
	push-uv i ‚Äî stack.append(uv[i])
	make-uv 1 x ‚Äî uv.append(x)

Maybe I should make all functions anonymous. That would certainly help with consistency. On the other hand, It would be nice if pure functions could be called normally so that a ton of near-empty closures aren't stored on the stack.

Let's try both.
`defun` will return a closure on the stack. It will then be popped unless it is explicitly saved by a `var` or `setq`. `lambda` will do the exact same thing, but it will not be given a name. This makes `defun` into a labeled lambda.

Added function scoping. Function scoping is different from normal scoping because it indicates the boundary of local variables. If a form breaks that boundary and references variables in a parent function, a closure must be formed to keep addressing from breaking.

`generator_expression` and `generator_compoundExpression` will need to know how to register an upvalue, and possibly allocate space on the stack for it.
`generator_setq` will need to know how to register and set an upvalue.

Compiler: Stack space needs to be allocated for an upvalue *before* the upvalue is actually used so that it can be treated as a function argument. The most direct way is to traverse the code tree beforehand to see if any upvalues are used. I'd rather not do this. A potential alternative is to create a VM instruction, `push-uv`, that will push the specified upvalue on the stack. This is less efficient if the upvalue is used multiple times since the original plan would do one push at the start of the function while this plan will push the upvalue every time it is used.

`push-closure` pushes a closure on the stack. A closure contains the function address and upvalue addresses.
`funcall` saves the current instruction pointer and upvalues, jumps to the new function, and sets the new upvalues as the current one.
`set-uv` sets an upvalue to the value of a stack object. The address of the upvalue is found by pointing to the correct upvalue address in the current upvalue array.
`push-uv` pushes an upvalue onto the stack. The address of the upvalue is found by pointing to the correct upvalue address in the current upvalue array.
`make-uv` allocates a new upvalue.

Why did I clear the locals_length in functions?
It is preventing me from accessing variables in parent scopes.
I suppose that is how it is supposed to work. All addressing is supposed to be done with an upvalue.
The problem is that an upvalue needs an index to point to. With multiple nested functions, it is impossible to determine the proper index of the free local.
Fixed. locals_length is no longer cleared in functions.

Statics can be added in a very similar way to upvalues. push-static, get-static, set-static

The upvalue list will actually be multiple. There will be one for each scope. When the scope is popped, so is the list. When the scope is initialized, the list is empty.
Each scope will have an upvalue trie. If a free variable is used in an inner scope, the current scope's trie will be searched for the upvalue index. If the index is nonexistent, a new upvalue will be created. The index will be inserted into the instruction that references the free variable.

These multiple arrays of upvalues will exist in both the compiler and the VM.

defun f x
        [Œª y
          (funcall y x)
       	 Œª z
       	   ‚Üê x z]
(funcall f 5)

compiles to

    f: jump e0
   Œª0: jump e1
   $y: push-uv uy
  $x1: push-index $x
       funcall $y $x1
	   return 1
  $c0: push-closure Œª0 1 x
e1,Œª1: jump e2
       set-uv ux z
	   return 1
  $c1: push-closure Œª1 1 x
   e2: push-list 2 $c0 $c1
       make-uv 1 x
	   return 3
e0,$x: push 5
       call f

Made big mistake. Variables are allocated and freed on the stack based on scope. What I should be doing is allocating and freeing locals only during function call and return.

Should I be creating closures for a variable in any parent scope?
I think the answer is "yes". Let's do that.
From now on, every scope may have upvalues. This may take more VM memory and increase program size, but it should make it more regular.

The first instance of an upvalue should register itself with the nearest function body above it if the free variable exists and is defined outside the function. Each scope can do some bookkeeping if needed to check if it has encountered that variable before.

Only create an upvalue if the free variable is defined outside of the current function.
Locals are kept track of per scope; upvalues are kept track of per function.
If the search for a variable remains inside the current function, do what you normally would do.
If the search for a variable crosses the function boundary, then create an upvalue.
If you find yourself creating an upvalue, first check to see if one already exists in the current function.
The upvalue is stored in the scope where the original variable was defined, but all containing functions will push closures referencing it.
When the scope containing the upvalue exits, it should move the local into the upvalue heap.

Parent scopes possess and manage upvalues.
Child functions create and capture upvalues.

`push-closure` is executed at the end of a function definition.
`funcall` is executed to trigger a function call.
`set-uv` is executed during function execution. Does not affect the stack.
`push-uv` is executed during function execution.
`make-uv` is executed at the end of a scope. Does not affect the stack.

Referencing non-local values will have zero overhead if the variable and the reference are in the same function.
Function definition will have overhead of one extra instruction.
Referencing an upvalue may add overhead.
Setting an upvalue should have zero overhead.
Exiting a scope will have overhead of one extra instruction if an upvalue is created.

I *think* I can implement this in the compiler now. It actually doesn't sound that hard.

`make-uv` doesn't actually make an upvalue. It might, but most of the time `push-closure` will do that. `make-uv` will just copy the local into the heap, so maybe it should be called something like `transfer-uv`?

`make-uv` and `push-closure` can both create upvalues.

`set-uv` and `push-uv` will both index upvalues by upvalue array address. This is an index into the function's array of upvalues.
`make-uv` and `push-closure` will both index upvalues by the index of the local variable. The index will be used as the key to access the upvalue address (not index), which will later be used during function calls to create the upvalue array, which is an array of pointers to the upvalues.

Creating a closure in the VM:
    There exists a stack of the same length as the data stack that contains pointers to upvalues.
    push-closure:
	    Arguments are indices of the free variables.
		Do a lookup of the variables in the upvalue stack. If an element corresponding to a free variable is null, create an upvalue and link it in the stack. Upvalues should point to the stack element that they were created for.
		Create the closure, storing the upvalue addresses.
	make-uv:
	    Arguments are indices of the free variables.
		Do a lookup of the variables in the upvalue stack. Copy the stack elements they correspond to into the upvalue heap (I think this means another implementation of GC).
	return:
	    Pop upvalues stack same number of times as data stack.

Compiler should be emitting all code required to read free variables.

`push-closure` must be inside the function scope so it can access the upvalues list. Function bypass label must come before `push-closure`, which means it and its jump must reside in the same scope. Function name label must come after the function bypass jump, so it must be in the same scope. Function name label must be outside the scope so that other functions can reference it.

And the answer? Temporarily break the scope abstraction. We can fix it later by emitting a `lambda` and a `var`.
Everything is kept in the local scope except the function name label.

Containers: objects, conses, upvalues
Closures are not on the list because they are contained in objects.

Functions must all become closures. The only valid call to a function is through a closure. The function label will become a gensym that is wholly contained by the function scope. The function name will be associated with an object instead of a label. Recursion might still be possible by sheer luck.

(keyword get-env ())
(keyword eval (exp env))
(keyword set-env ())
(
 (var x 5)
 (eval (quote (
			   (var y 3)
			   (+ x y)))
	   (get-env)))

(lambda (n)
  (if (= n 1)
	  1
	  (* n (self (1- n)))))

(defun let ((quote name) value (quote body))
  (eval (list ((quote var) name value)
			  body)
		(get-env)))

(let x 5
  (
   (setq x (1+ x))
   x))

(
 (var name (quote x))
 (var value 5)
 (var body (quote (
				   (setq x (1+ x))
				   x)))
 (eval (list ((quote var) name value)
			 body)
	   (get-env)))

(
 (eval (list ((quote var) (quote x) 5)
			 (quote (
					 (setq x (1+ x))
					 x)))
	   (get-env)))

Two types of macros?
(comptime (defmacro ...))
(runtime (defmacro ...))

Ideal duck-lisp reader: All steps happen at the same time. A single char is read at a time, and if a form is ready, it is compiled. Reader macros can call any function that is fully defined when that parser function is run. Macros can call any function that is fully defined by the time it is called.

duck-lisp v1.1: Functions are compiled as soon

Macros capture something. What do they capture?

(let ((x 5))
  (defexpander (lambda (body)
				 `(progn
					,@(mapcar (lambda () body) (number-sequence 1 x))))))

Macros themselves aren't a problem. Scoped macros that call functions aren't even that bad. The problem is that while functions can potentially exist at compile time, variables only have values at runtime.

Here's the root of the problem. What does the following code do? Perhaps `x' is a symbol at compiletime?

(runtime (
		  (var x 5)
		  (defun *2 (v) (* v 2))
		  (funcall (compiletime ((defun naughty () (*2 x)))))))

`*2' exists at both runtime and compiletime, primarily because it doesn't capture anything. If you want it to only exist at runtime, then change it to `(var *2 (lambda ...))`. `defun' is special.
`naughty' exists only at compiletime. It seems to me that the current environment should be the symbols names assigned to themselves.

First step is to redo how functions are compiled. Currently, when a function definition is found, high-level assembly is generated. In the new system, each function will be fully compiled to bytecode as soon as it is found. Function signatures will be added as well since those are nice. Maybe I'll add variadic functions too.
Second step is to implement macros, which should be easy once the above is done. A macro will be a normal function with a flag set that says it is a macro. It exists only at compile time. When a macro instance is encountered, just call the compiled functions on the code in the arguments and compile the result. Macros will be able to call existing functions, but only if they are not closures.


Every upvalue in the upvalue array call stack should always have an object on the stack associated with it.

Every transfer of a closure must result in an increment of the upvalue array reference count.
Every deletion of a closure must result in a decrement of the upvalue array reference count.
On a reference count of 0, the upvalue array must be deleted.


Function arity might not actually be that hard. If I do it in the VM, all I need is to add an extra field in the instruction for the number of arguments. It's almost as easy in the compiler. I just copy the length of the bindings expression and pass it to the assembler.
I already have a partial function signature. It's the one that I'm struggling to GC. Once I solve that, this addition will require little effort.
Funcall could pose a problem, but I think all I have to do is tell it the expected arity. If the arities match, then there's no problem. If they don't match, then an error is thrown.


I could assign every function an index, and then pass the number of functions to the VM. The VM would create an array of function signatures. These would then be used by function declarations without needing to ever free them.
Another option is to prepend all function signatures to the beginning of the bytecode. When the VM starts up, it creates a static (not C `static`) array of signatures. It then starts executing the bytecode. Function declarations will then point to the function signature instead of directly to the bytecode.
Arity could also be placed at the beginning of the function definition. A function call would jump to a portion in bytecode and then read the arity. This would not work as well for closures since the signature is also needed for `push-uv` and `set-uv`, so they would also need access to the definition.
Instead of doing a deep copy or shallow copy of closures, I could just keep references to them on the stack.
I could add garbage collection for upvalue arrays and ignore all this reference counting junk. It also allows me to limit the number of upvalue arrays that can be created. When I add string GC, I can probably copy most of the upvalue array GC since a string is just an array of chars.

GCing the upvalue arrays worked perfectly.


Free on compiled:
	duckLisp_register_label:duckLisp_t.labels
Free on quit:
    duckLisp_generator_quote:duckLisp_t.symbols_array
	duckLisp_generator_quote:duckLisp_t.symbols_trie


The slowest part of the language is the memory allocator.

Most memory freed. Some is still lost when the compiler throws an error.

Simple pattern matching macros shouldn't be too hard to add. Hygienic is not practical though.

(setq list (cdr list))

(defpmacro to (var op &rest args)
  (setq var (apply op var args)))

Need runtime arity for &rest to be practical.
Need a trie local to pmacros for variable expansion? Could do simple search-and-replace instead.

Unfortunately, this requires the same exact machinery as normal macros since the expansion happens apart from the definition.

If a function is pure (meaning it doesn't use any free variables and doesn't call any impure functions), then it could be compiled and saved for use by macros.

These can all fully define a bytecode stream:
source code
CST
AST
assembly + labels ‚Äî This one is annoying. Maybe I can merge these two? Labels are currently global, but could easily be generator and emitter parameters.
bytecode

These fully defined forms are useful to store and manipulate:
source code ‚Äî User input
AST ‚Äî True form of the language
assembly + labels ‚Äî Optional, but can be useful to interact with.
bytecode ‚Äî Executable code


Macros are easy to call in other macros because macros are defined in their own environment. Functions are not easy to call in macros because they are defined in a messy environment that doesn't actually exist at compile time.
Recursion in macros is easy because they are normal lambdas.


List types offer a layer of indirection between objects on the stack and conses on the heap. A cons should never appear on the stack, even though it is an object.
A non-null list should always point to a cons. It should never point to any other type.


Once everything is using one single heap, it should be a lot easier to make improvements, such as switching to a copying collector.

I spent a day debugging the VM and compiler since I thought there was a bug in it. I think I did fix a few potential bugs, some of which may have caused my original problems, but I think the final bug was forcing a garage collection from a running script. So that makes me think that there is a problem with the FFI.

Scratch that. Problem still remains. I'm guessing it's still GC related, and that maybe it has to do with the upvalues or upvalues array stack not being traced?

The bug disappeared when I went from this

(
 (defun 1 ‚Ä¶)
 (defun 2 ‚Ä¶)
 (defun 3 ‚Ä¶))

(
 (defun 1 ‚Ä¶)
 (defun 2 ‚Ä¶)
 (defun 3 ‚Ä¶))

to

(defun 1 ‚Ä¶)
(defun 2 ‚Ä¶)
(defun 3 ‚Ä¶)

(
 (defun 1 ‚Ä¶)
 (defun 2 ‚Ä¶)
 (defun 3 ‚Ä¶))


These problem numbers are buggy when done how I want to: 9 11


Definition: Pure functions are functions who's only free variables are C callbacks and pure functions. Recursion is permitted.


The bug is caused by creation of a corrupt upvalue array.
An object is created and collected multiple times. The first time it was allocated as an upvalue in "9.dl".
A closure is created who's upvalue array points to that upvalue, even though it was supposed to point to another upvalue.

I should try single-stepping when that closure is created.

Upvalue stack is corrupted.

Upvalue stack element 91 is a corrupted upvalue.

Watchpoint did not notice any change to `duckVM->upvalue_stack.elements[91]`. This makes me think that the element it's pointing to is what is getting corrupted, not the pointer itself.

Just to make sure, I checked the pointer value before and after the corruption, and the pointer didn't change.

The upvalue ended up on the free list.

An upvalue on the upvalue stack may not have any reference to it other than the upvalue stack itself.
On the other hand, the arrays on the upvalue array stack already have closures referencing them on the stack.

Bug is fixed. "9.dl" runs smoothly.


When the VM exits, the stack should be cleared, but all objects on the heap should remain until the next `_pushObject`.


Allowing pure function calls in macros is the final major feature I want in DL.


Using an arena allocator may have been a much better choice than the style of allocation I'm using now.


Each scope has a field that indicates if the first function encountered in a lower scope is pure (meaning it does not capture free variables). The only function that needs to set that field to a useful value is `defun`. The problem is by the time the generator for `defun` is called, the scope containing the "pure" annotation is gone. Somehow I need to thread that information from `lambda` through `var` to `defun`. One solution is to take the bodies of `var` and `lambda` out of their functions and turn them into new functions with an extra `*pure` argument. It doesn't feel right, but I suppose it doesn't *really* break the "generator-emitter" model.

We leave `defun` alone, except we call a different function for `var`.
We relocate the entire body of `var` to another function that has `*pure`.
We relocate the entire body of `lambda` to another function that has `*pure`.

And it turns out that I didn't need another field in `duckLisp_scope_t`. All I needed to do was check if the current closure captured anything, though I should somehow check to see if it captured itself.

Compilation of locals is greater than O(n). Every time a pure function is called, the body would have to be included in the binary, resulting in a ton of duplication. An alternative that is O(n) is to use static variables. All pure functions will have to be compiled twice, once for the final binary and once for use in macros.


Statics can be changed to be addressed either by name or by index. Any variable that has no definition will be treated as a named static variable. The compiler will issue a warning if this happens. If a static variable is registered with the compiler or declared beforehand the warning will not be issued and the static will be addressed by index instead of by name.

Statics may be created and destroyed at any time. Statics deleted during execution are simply marked deleted without freeing any memory.

Named and unnamed statics are almost one and the same. All unnamed statics must have a name.
Statics are objects that are pointed to by the statics array. Each entry has a string associated with it that acts as the name.
"Unnamed static" refers only to how the static is addressed. If the static is registered during compilation, it gets an entry in the unnamed statics array. References to statics that the compiler is explicitly told about are treated as unnamed.
When a static is deleted, it is only deleted from the statics array. Statics are never deleted from the unnamed statics array.

The statics array is traced by the garbage collector.
The unnamed statics array is traced by the garbage collector.

Unnamed references to statics are statically scoped. Named references to statics are dynamically scoped.


Macros use named references to statics for pure function calls.


Macros & pure functions and lexical scope are annoying.

Create a single VM instance to be used for all macro calls.
Pass a vector to the VM to trace for GC.
For each pure function:
    Compile each pure function to bytecode.
    Attach the bytecode to the associated symbol in the current scope.
    Execute the bytecode and store the result as a global variable.
    Put the result on the GC-traced vector.
For each macro call:
    If worried about memory: Delete all named global variables.
    For each pure function in the current scope:
        Add the closure object as a global variable.
    Call the macro function.


Lambdas stored in dynamic variables crash when called by bytecode they did not originate from.

1. A copy of the entire bytecode can be copied onto the heap.
2. A copy of the small pieces of bytecode that the function uses can be copied to the heap.

The former is probably better since function definitions may contain other function definitions, so the former reduces bytecode duplication.


Perhaps I should remove static access and only keep dynamic access? Deleting statics doesn't really work well with global closures. Global closures don't work well with dynamics either, but at least I can delete a dynamic and not have the closure address the wrong global. The VM halts if the requested global doesn't exist instead of silently using it as it would if the addressed global was a deleted static.

Final decision: Statics are fast, but must go.

How to improve space efficiency of dynamic variables? Maybe use symbols? Symbols mappings are never deleted, so I think this would actually work. I would just do it with an associative array.


Macros & pure functions should be a little less annoying.

Create a single VM instance to be used for all macro calls.
Pass a vector to the VM to trace for GC.
For each pure function:
    Compile each pure function to bytecode.
    Execute the bytecode.
    Put the result on the GC-traced vector.
    Attach the returned closure to the associated symbol in the current scope.
    Free the bytecode since the VM holds a copy.
For each macro call:
    For each pure function in the current scope from the root to the leaf:
        Add the closure object as a dynamic variable, overwriting any dynamics with the same name.
    Call the macro function.


New features:
    No more static variables
    Dynamic variables
    Garbage collected bytecode
    Tracing of user-added objects.

Static variables have been removed and dynamic/global variables have replaced them.


Bytecode is copied to the heap once on VM start.
The call stack may require references to the bytecode currently in use so that the VM knows which set of bytecode to execute when a function is called and returns.
New closures save a reference to the currently executing bytecode object.

If I were to create a standard duck-lisp, I would not include globals in the specification. This means that a compliant implementation would only need a minimal call stack like is currently implemented.

Bytecode copying and GC implemented.


User object tracing should be easy enough.


An alternative to adding objects to trace is to create a global vector that stores objects I want to keep alive. That would be a very Emacsy way of doing it. Since the GC isn't compacting, objects always stay in the same spot once they are allocated. Because of that, I can gensym the name and hold references to the global vector and all pure functions that I shove in it.

Create a single VM instance to be used for all macro calls.
Pass a vector to the VM to trace for GC.
For each pure function:
    Compile each pure function to bytecode.
    Execute the bytecode.
    Put the result on the GC-traced vector.
    Attach the returned closure to the associated symbol in the current scope.
    Free the bytecode since the VM holds a copy.
For each macro call:
    For each pure function in the current scope from the root to the leaf:
        Add the closure object as a dynamic variable, overwriting any dynamics with the same name.
    Call the macro function.
Destroy the VM.

The GC-traced vector is actually a nested list. A new cons is added to the list each time a new scope is created. Each element holds a list of the pure closures.

It looks like there is already an array of DL functions in each scope. Currently they hold a copy of the bytecode. They will need to be changed to hold a reference to the closure.
Macros are the same way, but they actually work, so I'm going to leave them alone for now.

The DL scope stack only exists to prevent garbage collection of the closures.


Nope. It's still hard.

The problem is that I want to create closures, which means I can't simply use dynamic scope for all the pure functions. Using dynamic scope for lexically scoped functions won't work.
On the other hand, I now have the ability to hold closures on the scope stack, so maybe there's a way I can use actual lexical scope?


I think running callbacks in macros is easy. I should just need to copy over the symbol table and then delete it before I call duckLisp_quit.


The duckVM scope stack along with its closures will stay, but I will use genuine lexical scoping. I will copy over the compiler scope stack to the sub-compiler and then have it compile a single pure function that may reference other functions. This will create a small chunk of bytecode that is then run to create a closure. The closure is placed on the VM scope stack. When a macro is called, then entirety of the VM scope stack is manually placed on the VM's stack and the macro is run.
The only problem I can see right now is that the upvalues might need to reference objects with pointers, but if I recall correctly, they instead use indices which shouldn't cause a problem.


Next step: Do some cleanup, fix bugs, and round out the keywords.

Defer:

Todo:
	macros
	Clean up.
	Profile.
	Optimize?
	Clean up.
	Merge with Hidey-Chess and add required functions.
	Clean up.
